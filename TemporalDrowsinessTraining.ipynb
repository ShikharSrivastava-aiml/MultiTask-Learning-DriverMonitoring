{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, random, torch, glob\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# ====== Temporal Drowsiness Dataset ======\n",
    "class TemporalDrowsinessDataset(Dataset):\n",
    "    \"\"\"Temporal dataset for blinking detection\"\"\"\n",
    "    def __init__(self, subject_dirs, transform=None, sequence_length=5, stride=2):\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "\n",
    "        print(f\"Building temporal dataset with window={sequence_length}, stride={stride}\")\n",
    "        valid_sequences = 0\n",
    "        skipped_sequences = 0\n",
    "\n",
    "        for subject_dir in subject_dirs:\n",
    "            frames_dir = os.path.join(subject_dir, \"frames\")\n",
    "            csv_path = os.path.join(frames_dir, \"labels.csv\")\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df = df.sort_values('filename')\n",
    "\n",
    "            for i in range(0, len(df) - sequence_length + 1, stride):\n",
    "                seq_files = []\n",
    "                seq_labels = []\n",
    "                all_valid = True\n",
    "\n",
    "                for j in range(i, i + sequence_length):\n",
    "                    img_path = os.path.join(frames_dir, df.iloc[j]['filename'])\n",
    "\n",
    "                    if os.path.exists(img_path):\n",
    "                        try:\n",
    "                            with Image.open(img_path) as img:\n",
    "                                img.verify()\n",
    "                            seq_files.append(img_path)\n",
    "                            seq_labels.append(df.iloc[j]['label'])\n",
    "                        except:\n",
    "                            all_valid = False\n",
    "                            break\n",
    "                    else:\n",
    "                        all_valid = False\n",
    "                        break\n",
    "\n",
    "                if all_valid and len(seq_files) == sequence_length:\n",
    "                    # Majority vote for label\n",
    "                    target_label = 1 if sum(seq_labels) > len(seq_labels) // 2 else 0\n",
    "                    self.samples.append((seq_files, target_label, seq_labels))\n",
    "                    valid_sequences += 1\n",
    "                else:\n",
    "                    skipped_sequences += 1\n",
    "\n",
    "        print(f\"Created {valid_sequences} valid sequences, skipped {skipped_sequences}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_files, label, _ = self.samples[idx]\n",
    "        frames = []\n",
    "\n",
    "        for img_path in seq_files:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                frames.append(image)\n",
    "            except:\n",
    "                # Fallback to black frame\n",
    "                if self.transform:\n",
    "                    black_image = Image.new('RGB', (256, 256), color='black')\n",
    "                    image = self.transform(black_image)\n",
    "                else:\n",
    "                    image = torch.zeros(3, 256, 256)\n",
    "                frames.append(image)\n",
    "\n",
    "        frames = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        return frames, label\n",
    "\n",
    "# ====== Temporal Blinking Model ======\n",
    "class TemporalBlinkingModel(nn.Module):\n",
    "    def __init__(self, backbone_name='mobilevit_s', pretrained=True,\n",
    "                 num_classes=2, sequence_length=5, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spatial feature extractor\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "\n",
    "        # Temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            feat_dim, hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim * 2,  # Bidirectional\n",
    "            num_heads=4,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extract spatial features for each frame\"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "\n",
    "        features = self.backbone.forward_features(x)\n",
    "        features = features.mean(dim=[2, 3])  # Global average pooling\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, channels, height, width]\n",
    "        features = self.extract_features(x)\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        lstm_out, (h_n, c_n) = self.lstm(features)\n",
    "\n",
    "        # Apply attention\n",
    "        lstm_out = lstm_out.transpose(0, 1)  # [seq_len, batch, features]\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        attn_out = attn_out.transpose(0, 1)  # [batch, seq_len, features]\n",
    "\n",
    "        # Temporal aggregation\n",
    "        temporal_features = attn_out.mean(dim=1)  # Mean pooling\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(temporal_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "# ====== Training Functions ======\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for frames, labels in tqdm(loader, desc=\"Training\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(loader, desc=\"Validating\"):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader),  100. * correct / total, all_preds, all_labels\n",
    "\n",
    "# ====== Visualization ======\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax1.plot(train_losses, label='Train Loss')\n",
    "    ax1.plot(val_losses, label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(train_accs, label='Train Acc')\n",
    "    ax2.plot(val_accs, label='Val Acc')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('blinking_training_history.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=['Not Blinking', 'Blinking']):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix - Blinking Detection')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('blinking_confusion_matrix.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# ====== Main Training Script ======\n",
    "def main():\n",
    "    # Configuration\n",
    "    SEQUENCE_LENGTH = 5\n",
    "    STRIDE = 2\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    # Device selection (MPS for Mac, CUDA for NVIDIA, CPU fallback)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"Using Apple Metal Performance Shaders (MPS)\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Data transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Dataset paths\n",
    "    drowsy_base = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Drowsiness/S5\"\n",
    "\n",
    "    # Get subject directories\n",
    "    subjects = [d for d in os.listdir(drowsy_base)\n",
    "                if os.path.isdir(os.path.join(drowsy_base, d)) and d.isdigit()]\n",
    "    subjects = sorted(subjects, key=int)\n",
    "\n",
    "    print(f\"\\nFound {len(subjects)} subjects: {subjects}\")\n",
    "\n",
    "    # Subject-wise split (70-15-15)\n",
    "    random.seed(42)\n",
    "    random.shuffle(subjects)\n",
    "\n",
    "    n_train = int(len(subjects) * 0.7)\n",
    "    n_val = int(len(subjects) * 0.15)\n",
    "\n",
    "    train_subjects = subjects[:n_train]\n",
    "    val_subjects = subjects[n_train:n_train + n_val]\n",
    "    test_subjects = subjects[n_train + n_val:]\n",
    "\n",
    "    print(f\"\\nSplit - Train: {train_subjects}, Val: {val_subjects}, Test: {test_subjects}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dirs = [os.path.join(drowsy_base, s) for s in train_subjects]\n",
    "    val_dirs = [os.path.join(drowsy_base, s) for s in val_subjects]\n",
    "    test_dirs = [os.path.join(drowsy_base, s) for s in test_subjects]\n",
    "\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = TemporalDrowsinessDataset(train_dirs, transform_train, SEQUENCE_LENGTH, STRIDE)\n",
    "    val_dataset = TemporalDrowsinessDataset(val_dirs, transform_val, SEQUENCE_LENGTH, STRIDE)\n",
    "    test_dataset = TemporalDrowsinessDataset(test_dirs, transform_val, SEQUENCE_LENGTH, STRIDE)\n",
    "\n",
    "    print(f\"\\nDataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = TemporalBlinkingModel(\n",
    "        backbone_name='mobilevit_s',\n",
    "        pretrained=True,\n",
    "        num_classes=2,\n",
    "        sequence_length=SEQUENCE_LENGTH\n",
    "    ).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "    # Weighted loss for class imbalance\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0]).to(device))\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_blinking_model.pth')\n",
    "            print(f\"Best model saved with Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "    # Test evaluation\n",
    "    print(\"\\n=== Testing Best Model ===\")\n",
    "    model.load_state_dict(torch.load('best_blinking_model.pth'))\n",
    "    test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds,\n",
    "                              target_names=['Not Blinking', 'Blinking']))\n",
    "\n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Model saved as: best_blinking_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a9870975d36dde1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
