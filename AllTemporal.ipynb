{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T10:15:13.977256600Z",
     "start_time": "2026-02-10T10:15:03.229267900Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. UNIVERSAL TEMPORAL DATASET\n",
    "# ==========================================\n",
    "class UniversalTemporalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A robust dataset class that handles image sequences for Gaze, Face, and Drowsiness.\n",
    "    It expects a list of (image_path, label) tuples that are ALREADY sorted by time.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, transform=None, sequence_length=32, stride=8):\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "\n",
    "        # Build sequences with sliding window\n",
    "        # We assume 'samples' is a list of (path, label) sorted by time\n",
    "        for i in range(0, len(samples) - sequence_length + 1, stride):\n",
    "            window = samples[i : i + sequence_length]\n",
    "\n",
    "            # extract paths and labels\n",
    "            paths = [item[0] for item in window]\n",
    "            labels = [item[1] for item in window]\n",
    "\n",
    "            # Integrity check: Ensure files exist (optimization: check lazily or first few)\n",
    "            # For speed, we assume the splitter provided valid paths.\n",
    "\n",
    "            # Determine label for the sequence\n",
    "            # For Face: Label is constant.\n",
    "            # For Gaze/Drowsy: Use Majority Vote or Last Frame.\n",
    "            # We use Majority Vote here for stability.\n",
    "            target_label = max(set(labels), key=labels.count)\n",
    "\n",
    "            self.sequences.append((paths, target_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_paths, label = self.sequences[idx]\n",
    "        frames = []\n",
    "\n",
    "        for p in img_paths:\n",
    "            try:\n",
    "                # Load Image\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                frames.append(img)\n",
    "            except Exception as e:\n",
    "                # Black frame fallback for corruption\n",
    "                # print(f\"Error loading {p}: {e}\")\n",
    "                frames.append(torch.zeros(3, 256, 256))\n",
    "\n",
    "        # Stack to [T, C, H, W]\n",
    "        try:\n",
    "            frames = torch.stack(frames, dim=0)\n",
    "        except:\n",
    "            # Fallback if transforms failed completely\n",
    "            frames = torch.zeros(self.sequence_length, 3, 256, 256)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "# Helper to prepare file lists\n",
    "def get_face_files_sorted(subject_dir):\n",
    "    \"\"\"Returns sorted list of (path, label_id) for a subject\"\"\"\n",
    "    # Assuming subject_dir ends in subject ID (e.g., .../01)\n",
    "    try:\n",
    "        subject_id = int(os.path.basename(subject_dir)) - 1 # 0-indexed\n",
    "    except:\n",
    "        subject_id = 0\n",
    "\n",
    "    valid_dir = os.path.join(subject_dir, \"Frames_RGB_Valid\")\n",
    "    if not os.path.exists(valid_dir):\n",
    "        return []\n",
    "\n",
    "    files = sorted(glob.glob(os.path.join(valid_dir, \"*.jpg\")))\n",
    "    return [(f, subject_id) for f in files]\n",
    "\n",
    "def get_gaze_files_sorted(subject_dir):\n",
    "    \"\"\"Reads CSV and returns sorted list of (path, gaze_label)\"\"\"\n",
    "    # Adjust paths based on your structure\n",
    "    try:\n",
    "        subj_name = os.path.basename(subject_dir)\n",
    "        valid_dir = os.path.join(subject_dir, \"Frames_RGB_Valid\")\n",
    "        csv_path = os.path.join(subject_dir, f\"Valid_gaze_label_{subj_name}.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path) or not os.path.exists(valid_dir):\n",
    "            return []\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Ensure mapping matches file structure\n",
    "        # df usually has 'Frame Index' and 'Gaze Zone'\n",
    "        samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            idx = int(row['Frame Index'])\n",
    "            label = int(row['Gaze Zone'])\n",
    "            fname = f\"frame_{idx:04d}.jpg\"\n",
    "            fpath = os.path.join(valid_dir, fname)\n",
    "            if os.path.exists(fpath):\n",
    "                samples.append((fpath, label))\n",
    "        return samples\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_drowsy_files_sorted(subject_dir):\n",
    "    \"\"\"Reads CSV and returns sorted list of (path, label)\"\"\"\n",
    "    frames_dir = os.path.join(subject_dir, \"frames\")\n",
    "    csv_path = os.path.join(frames_dir, \"labels.csv\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        return []\n",
    "\n",
    "    df = pd.read_csv(csv_path).sort_values('filename')\n",
    "    samples = []\n",
    "    for _, row in df.iterrows():\n",
    "        fpath = os.path.join(frames_dir, row['filename'])\n",
    "        if os.path.exists(fpath):\n",
    "            samples.append((fpath, int(row['label'])))\n",
    "    return samples\n",
    "\n",
    "# ==========================================\n",
    "# 2. FULLY TEMPORAL MODEL\n",
    "# ==========================================\n",
    "class FullyTemporalMultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone_name='mobilevit_s', pretrained=True,\n",
    "                 gaze_classes=9, drowsy_classes=2, face_classes=15,\n",
    "                 hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Shared Spatial Backbone\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "\n",
    "        # 2. Temporal Processing (LSTM)\n",
    "        # We use separate LSTMs because the temporal dynamics differ:\n",
    "        # - Gaze: Saccadic, quick changes\n",
    "        # - Drowsy: Slow/Patterned (blinking sequence)\n",
    "        # - Face: Constant (Identity doesn't change), but video helps smooth noise\n",
    "\n",
    "        self.gaze_lstm = nn.LSTM(feat_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.drowsy_lstm = nn.LSTM(feat_dim, hidden_dim, batch_first=True, bidirectional=True) # BiDir for blink context\n",
    "        self.face_lstm = nn.LSTM(feat_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # 3. Heads\n",
    "        self.gaze_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, gaze_classes)\n",
    "        )\n",
    "\n",
    "        self.drowsy_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 2), # *2 for Bidirectional\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, drowsy_classes)\n",
    "        )\n",
    "\n",
    "        self.face_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, face_classes)\n",
    "        )\n",
    "\n",
    "    def extract_spatial(self, x):\n",
    "        # x: [B, T, C, H, W]\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        feats = self.backbone.forward_features(x) # [B*T, Feat, H_grid, W_grid]\n",
    "        feats = feats.mean(dim=[2, 3]) # Global Average Pooling -> [B*T, Feat]\n",
    "        feats = feats.view(b, t, -1)   # Reshape back to sequence -> [B, T, Feat]\n",
    "        return feats\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        # 1. Spatial Features\n",
    "        feats = self.extract_spatial(x) # [B, T, Feat]\n",
    "\n",
    "        if task == 'gaze':\n",
    "            # Run LSTM\n",
    "            out, (hn, cn) = self.gaze_lstm(feats)\n",
    "            # Use the feature from the LAST frame in sequence (most recent gaze)\n",
    "            last_seq_feat = out[:, -1, :]\n",
    "            return self.gaze_head(last_seq_feat)\n",
    "\n",
    "        elif task == 'drowsy':\n",
    "            out, (hn, cn) = self.drowsy_lstm(feats)\n",
    "            # For drowsiness, the whole pattern matters. We can pool the LSTM output.\n",
    "            # Mean pooling over time captures the \"event\" (blink) anywhere in window\n",
    "            seq_feat = out.mean(dim=1)\n",
    "            return self.drowsy_head(seq_feat)\n",
    "\n",
    "        elif task == 'face':\n",
    "            out, (hn, cn) = self.face_lstm(feats)\n",
    "            # Face ID is constant. Mean pooling reduces noise/occlusion.\n",
    "            seq_feat = out.mean(dim=1)\n",
    "            return self.face_head(seq_feat)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN SCRIPT\n",
    "# ==========================================\n",
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.data = []\n",
    "        for task, ds in datasets.items():\n",
    "            if ds: self.data += [(task, i) for i in range(len(ds))]\n",
    "        self.datasets = datasets\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        task, i = self.data[idx]\n",
    "        x, y = self.datasets[task][i]\n",
    "        return x, y, task\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings\n",
    "    WINDOW_SIZE = 16\n",
    "    STRIDE = 8\n",
    "    BATCH_SIZE = 1 # Lower batch size because [B, 5, 3, 256, 256] is heavy\n",
    "    EPOCHS = 20\n",
    "    LR = 1e-4\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # Slightly smaller for sequence efficiency\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(f\"Initializing Sequence Training (Window={WINDOW_SIZE})...\")\n",
    "\n",
    "    # --- DATA PATHS ---\n",
    "    base_gaze_face = r\"C:\\Users\\shikh\\OneDrive - The University of Western Ontario\\Desktop\\Thesis\\MultiTask\\Gaze\\S6_face_RGB\"\n",
    "    base_drowsy = r\"C:\\Users\\shikh\\OneDrive - The University of Western Ontario\\Desktop\\Thesis\\MultiTask\\Drowsiness\\S5\"\n",
    "\n",
    "    # --- 1. PREPARE DROWSINESS (Subject-wise Split) ---\n",
    "    # Drowsiness MUST stay subject-wise to avoid learning personal blink patterns\n",
    "    print(\"Preparing Drowsiness Data...\")\n",
    "    drowsy_subjs = sorted([d for d in os.listdir(base_drowsy) if d.isdigit()], key=int)\n",
    "    random.seed(42); random.shuffle(drowsy_subjs)\n",
    "\n",
    "    split1 = int(0.7 * len(drowsy_subjs))\n",
    "    split2 = int(0.85 * len(drowsy_subjs))\n",
    "\n",
    "    drowsy_train_files, drowsy_val_files, drowsy_test_files = [], [], []\n",
    "\n",
    "    for s in drowsy_subjs[:split1]: drowsy_train_files.extend(get_drowsy_files_sorted(os.path.join(base_drowsy, s)))\n",
    "    for s in drowsy_subjs[split1:split2]: drowsy_val_files.extend(get_drowsy_files_sorted(os.path.join(base_drowsy, s)))\n",
    "    for s in drowsy_subjs[split2:]: drowsy_test_files.extend(get_drowsy_files_sorted(os.path.join(base_drowsy, s)))\n",
    "\n",
    "    # --- 2. PREPARE GAZE (Subject-wise Split) ---\n",
    "    # Gaze generalizes better with subject-wise split\n",
    "    print(\"Preparing Gaze Data...\")\n",
    "    gaze_subjs = sorted([d for d in os.listdir(base_gaze_face) if d.isdigit()], key=int)\n",
    "    random.seed(42); random.shuffle(gaze_subjs)\n",
    "\n",
    "    gaze_train_files, gaze_val_files, gaze_test_files = [], [], []\n",
    "\n",
    "    for s in gaze_subjs[:split1]: gaze_train_files.extend(get_gaze_files_sorted(os.path.join(base_gaze_face, s)))\n",
    "    for s in gaze_subjs[split1:split2]: gaze_val_files.extend(get_gaze_files_sorted(os.path.join(base_gaze_face, s)))\n",
    "    for s in gaze_subjs[split2:]: gaze_test_files.extend(get_gaze_files_sorted(os.path.join(base_gaze_face, s)))\n",
    "\n",
    "    # --- 3. PREPARE FACE (Within-Subject Split) ---\n",
    "    # Face recognition needs to see the person in Train to recognize in Test\n",
    "    print(\"Preparing Face Data (Within-Subject Split)...\")\n",
    "    face_train_files, face_val_files, face_test_files = [], [], []\n",
    "\n",
    "    # Iterate ALL subjects available in the folder\n",
    "    all_face_subjs = sorted([d for d in os.listdir(base_gaze_face) if d.isdigit()], key=int)\n",
    "\n",
    "    for s in all_face_subjs:\n",
    "        s_path = os.path.join(base_gaze_face, s)\n",
    "        # Get all files for this person, sorted by time/filename\n",
    "        person_files = get_face_files_sorted(s_path)\n",
    "\n",
    "        if len(person_files) < 10: continue # Skip if too few frames\n",
    "\n",
    "        # Split THIS person's data 70/15/15\n",
    "        n = len(person_files)\n",
    "        n_tr = int(n * 0.7)\n",
    "        n_va = int(n * 0.15)\n",
    "\n",
    "        face_train_files.extend(person_files[:n_tr])\n",
    "        face_val_files.extend(person_files[n_tr : n_tr + n_va])\n",
    "        face_test_files.extend(person_files[n_tr + n_va:])\n",
    "\n",
    "    # --- BUILD DATASETS ---\n",
    "    datasets = {\n",
    "        'train': {\n",
    "            'gaze': UniversalTemporalDataset(gaze_train_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'drowsy': UniversalTemporalDataset(drowsy_train_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'face': UniversalTemporalDataset(face_train_files, transform, WINDOW_SIZE, STRIDE)\n",
    "        },\n",
    "        'val': {\n",
    "            'gaze': UniversalTemporalDataset(gaze_val_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'drowsy': UniversalTemporalDataset(drowsy_val_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'face': UniversalTemporalDataset(face_val_files, transform, WINDOW_SIZE, STRIDE)\n",
    "        },\n",
    "        'test': {\n",
    "            'gaze': UniversalTemporalDataset(gaze_test_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'drowsy': UniversalTemporalDataset(drowsy_test_files, transform, WINDOW_SIZE, STRIDE),\n",
    "            'face': UniversalTemporalDataset(face_test_files, transform, WINDOW_SIZE, STRIDE)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"\\nDataset Sizes (Sequences):\")\n",
    "    print(f\"  Train: Gaze={len(datasets['train']['gaze'])}, Drowsy={len(datasets['train']['drowsy'])}, Face={len(datasets['train']['face'])}\")\n",
    "    print(f\"  Val:   Gaze={len(datasets['val']['gaze'])}, Drowsy={len(datasets['val']['drowsy'])}, Face={len(datasets['val']['face'])}\")\n",
    "\n",
    "    # --- DATALOADERS ---\n",
    "    train_loaders = {\n",
    "        k: DataLoader(v, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        for k, v in datasets['train'].items() if len(v) > 0\n",
    "    }\n",
    "\n",
    "    val_loader = DataLoader(MixedDataset(datasets['val']), batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(MixedDataset(datasets['test']), batch_size=1, shuffle=False)\n",
    "\n",
    "    # --- MODEL & OPTIMIZER ---\n",
    "    model = FullyTemporalMultiTaskModel(face_classes=16).to(DEVICE) # Ensure face_classes matches dataset (1-15 -> 16 slots)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- LOGGING SETUP ---\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc_gaze': [],\n",
    "        'val_acc_drowsy': [],\n",
    "        'val_acc_face': []\n",
    "    }\n",
    "\n",
    "    # --- TRAINING LOOP ---\n",
    "    print(\"\\nStarting Fully Temporal Training...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        batches = 0\n",
    "\n",
    "        # Iterate tasks in round-robin or random mix\n",
    "        active_tasks = list(train_loaders.keys())\n",
    "        iters = {t: iter(train_loaders[t]) for t in active_tasks}\n",
    "\n",
    "        # Train for N steps (based on largest dataset)\n",
    "        steps = max([len(l) for l in train_loaders.values()])\n",
    "\n",
    "        pbar = tqdm(range(steps), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for _ in pbar:\n",
    "            step_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for task in active_tasks:\n",
    "                try:\n",
    "                    x, y = next(iters[task])\n",
    "                except StopIteration:\n",
    "                    iters[task] = iter(train_loaders[task])\n",
    "                    x, y = next(iters[task])\n",
    "\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "                out = model(x, task)\n",
    "                loss = criterion(out, y)\n",
    "                loss.backward()\n",
    "                step_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_train_loss += step_loss / len(active_tasks)\n",
    "            batches += 1\n",
    "            pbar.set_postfix({'loss': total_train_loss/batches})\n",
    "\n",
    "        avg_train_loss = total_train_loss / batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        val_loss_sum = 0\n",
    "        val_samples = 0\n",
    "        correct = {'gaze':0, 'drowsy':0, 'face':0}\n",
    "        total = {'gaze':0, 'drowsy':0, 'face':0}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y, task in val_loader:\n",
    "                # x is [1, T, C, H, W] via MixedDataset batch_size=1\n",
    "                # But DataLoader adds a batch dim, so if batch_size=1 it matches.\n",
    "                # If MixedDataset returns tuple, collate might stack them.\n",
    "                # Here, batch_size=1 in Loader means x is [1, T, C, H, W] directly if collated\n",
    "\n",
    "                # Handling MixedDataset tuple return:\n",
    "                # x tuple of tensors if batch > 1. Here batch=1.\n",
    "                x = x[0].unsqueeze(0).to(DEVICE) # Ensure [1, T, C, H, W]\n",
    "                y = y[0].unsqueeze(0).to(DEVICE)\n",
    "                t_name = task[0]\n",
    "\n",
    "                out = model(x, t_name)\n",
    "                loss = criterion(out, y)\n",
    "                val_loss_sum += loss.item()\n",
    "                val_samples += 1\n",
    "\n",
    "                pred = out.argmax(1)\n",
    "                correct[t_name] += (pred == y).sum().item()\n",
    "                total[t_name] += 1\n",
    "\n",
    "        avg_val_loss = val_loss_sum / max(1, val_samples)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        acc_g = 100 * correct['gaze'] / max(1, total['gaze'])\n",
    "        acc_d = 100 * correct['drowsy'] / max(1, total['drowsy'])\n",
    "        acc_f = 100 * correct['face'] / max(1, total['face'])\n",
    "\n",
    "        history['val_acc_gaze'].append(acc_g)\n",
    "        history['val_acc_drowsy'].append(acc_d)\n",
    "        history['val_acc_face'].append(acc_f)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Results:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracies: Gaze={acc_g:.2f}% | Drowsy={acc_d:.2f}% | Face={acc_f:.2f}%\")\n",
    "\n",
    "        # --- SAVE PLOTS EVERY EPOCH ---\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "        plt.title('Loss History')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['val_acc_gaze'], label='Gaze Acc')\n",
    "        plt.plot(history['val_acc_drowsy'], label='Drowsy Acc')\n",
    "        plt.plot(history['val_acc_face'], label='Face Acc')\n",
    "        plt.title('Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'training_curves_epoch_{epoch+1}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save Checkpoint\n",
    "        torch.save(model.state_dict(), f\"hybrid_temporal_model_ep{epoch+1}.pth\")\n",
    "\n",
    "    # --- FINAL TEST ---\n",
    "    print(\"\\nRunning Final Test...\")\n",
    "    model.eval()\n",
    "    t_correct = {'gaze':0, 'drowsy':0, 'face':0}\n",
    "    t_total = {'gaze':0, 'drowsy':0, 'face':0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, task in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x = x[0].unsqueeze(0).to(DEVICE)\n",
    "            y = y[0].unsqueeze(0).to(DEVICE)\n",
    "            t_name = task[0]\n",
    "            out = model(x, t_name)\n",
    "            pred = out.argmax(1)\n",
    "            t_correct[t_name] += (pred == y).sum().item()\n",
    "            t_total[t_name] += 1\n",
    "\n",
    "    print(\"FINAL TEST RESULTS:\")\n",
    "    for t in ['gaze', 'drowsy', 'face']:\n",
    "        acc = 100 * t_correct[t] / max(1, t_total[t])\n",
    "        print(f\"  {t.upper()}: {acc:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sequence Training (Window=16)...\n",
      "Preparing Drowsiness Data...\n",
      "Preparing Gaze Data...\n",
      "Preparing Face Data (Within-Subject Split)...\n",
      "\n",
      "Dataset Sizes (Sequences):\n",
      "  Train: Gaze=2962, Drowsy=6808, Face=3121\n",
      "  Val:   Gaze=618, Drowsy=1347, Face=667\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 48.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 334\u001B[0m\n\u001B[0;32m    331\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(MixedDataset(datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    333\u001B[0m \u001B[38;5;66;03m# --- MODEL & OPTIMIZER ---\u001B[39;00m\n\u001B[1;32m--> 334\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mFullyTemporalMultiTaskModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mface_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Ensure face_classes matches dataset (1-15 -> 16 slots)\u001B[39;00m\n\u001B[0;32m    335\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLR, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m    336\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1352\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1353\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1355\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 915\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    919\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    920\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 915\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    919\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    920\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001B[0m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 915\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    919\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    920\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    938\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    939\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    940\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 942\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    943\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    945\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\Documents\\Code\\MultiTask-Learning-DriverMonitoring\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1335\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1336\u001B[0m             device,\n\u001B[0;32m   1337\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1338\u001B[0m             non_blocking,\n\u001B[0;32m   1339\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1340\u001B[0m         )\n\u001B[1;32m-> 1341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1345\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1347\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 48.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a2a5b7263b5bded"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
