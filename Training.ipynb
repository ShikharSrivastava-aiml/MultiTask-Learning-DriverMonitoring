{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-02-03T08:15:36.429380Z"
    }
   },
   "source": [
    "import os, random, torch, glob\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ====== Temporal Dataset Classes ======\n",
    "class TemporalGazeDataset(Dataset):\n",
    "    \"\"\"Gaze dataset with temporal context (sliding window)\"\"\"\n",
    "    def __init__(self, image_dir, label_csv, transform=None, sequence_length=5):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = pd.read_csv(label_csv).sort_values('Frame Index')\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(idx, idx + self.sequence_length):\n",
    "            frame_index = self.labels_df.iloc[i]['Frame Index']\n",
    "            gaze_label = int(self.labels_df.iloc[i]['Gaze Zone'])\n",
    "            image_path = os.path.join(self.image_dir, f\"frame_{int(frame_index):04d}.jpg\")\n",
    "\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            frames.append(image)\n",
    "            labels.append(gaze_label)\n",
    "\n",
    "        # Stack frames along channel dimension for temporal processing\n",
    "        frames = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        # Return middle frame's label as target (or last frame for prediction)\n",
    "        return frames, labels[self.sequence_length // 2]\n",
    "\n",
    "class TemporalDrowsinessDataset(Dataset):\n",
    "    \"\"\"Drowsiness dataset with temporal context for blinking detection\"\"\"\n",
    "    def __init__(self, subject_dirs, transform=None, sequence_length=5):\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.samples = []\n",
    "\n",
    "        for subject_dir in subject_dirs:\n",
    "            frames_dir = os.path.join(subject_dir, \"frames\")\n",
    "            csv_path = os.path.join(frames_dir, \"labels.csv\")\n",
    "\n",
    "            if os.path.exists(csv_path):\n",
    "                df = pd.read_csv(csv_path)\n",
    "                # Sort by filename to maintain temporal order\n",
    "                df = df.sort_values('filename')\n",
    "\n",
    "                for i in range(len(df) - sequence_length + 1):\n",
    "                    seq_files = []\n",
    "                    seq_labels = []\n",
    "                    valid = True\n",
    "\n",
    "                    for j in range(i, i + sequence_length):\n",
    "                        img_path = os.path.join(frames_dir, df.iloc[j]['filename'])\n",
    "                        if os.path.exists(img_path):\n",
    "                            seq_files.append(img_path)\n",
    "                            seq_labels.append(df.iloc[j]['label'])\n",
    "                        else:\n",
    "                            valid = False\n",
    "                            break\n",
    "\n",
    "                    if valid:\n",
    "                        # Use middle frame's label or majority vote\n",
    "                        target_label = seq_labels[sequence_length // 2]\n",
    "                        self.samples.append((seq_files, target_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_files, label = self.samples[idx]\n",
    "        frames = []\n",
    "\n",
    "        for img_path in seq_files:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            frames.append(image)\n",
    "\n",
    "        frames = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        return frames, label\n",
    "\n",
    "class TemporalFaceDataset(Dataset):\n",
    "    \"\"\"Face recognition with temporal context\"\"\"\n",
    "    def __init__(self, samples, transform, sequence_length=5):\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.temporal_samples = []\n",
    "\n",
    "        # Group samples by person\n",
    "        person_samples = {}\n",
    "        for img_path, label in samples:\n",
    "            if label not in person_samples:\n",
    "                person_samples[label] = []\n",
    "            person_samples[label].append(img_path)\n",
    "\n",
    "        # Create temporal sequences\n",
    "        for label, paths in person_samples.items():\n",
    "            paths.sort()  # Ensure temporal ordering\n",
    "            for i in range(len(paths) - sequence_length + 1):\n",
    "                seq = paths[i:i + sequence_length]\n",
    "                self.temporal_samples.append((seq, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.temporal_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_paths, label = self.temporal_samples[idx]\n",
    "        frames = []\n",
    "\n",
    "        for img_path in seq_paths:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            frames.append(image)\n",
    "\n",
    "        frames = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        return frames, label\n",
    "\n",
    "# ====== Temporal Multi-Task Model ======\n",
    "class TemporalMultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone_name='mobilevit_s', pretrained=True,\n",
    "                 gaze_classes=9, drowsy_classes=2, face_classes=15,\n",
    "                 sequence_length=5, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spatial feature extractor (shared backbone)\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "\n",
    "        # Temporal processing modules (task-specific)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # LSTM for temporal modeling (task-specific)\n",
    "        self.gaze_lstm = nn.LSTM(feat_dim, hidden_dim, num_layers=2,\n",
    "                                  batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.drowsy_lstm = nn.LSTM(feat_dim, hidden_dim, num_layers=2,\n",
    "                                    batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.face_lstm = nn.LSTM(feat_dim, hidden_dim, num_layers=2,\n",
    "                                  batch_first=True, dropout=0.3, bidirectional=True)\n",
    "\n",
    "        # Temporal attention modules\n",
    "        self.gaze_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.2)\n",
    "        self.drowsy_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.2)\n",
    "        self.face_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.2)\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.gaze_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, gaze_classes)\n",
    "        )\n",
    "\n",
    "        self.drowsy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, drowsy_classes)\n",
    "        )\n",
    "\n",
    "        self.face_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, face_classes)\n",
    "        )\n",
    "\n",
    "    def extract_spatial_features(self, x):\n",
    "        \"\"\"Extract spatial features for each frame in the sequence\"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "\n",
    "        # Extract features using backbone\n",
    "        features = self.backbone.forward_features(x)\n",
    "        features = features.mean(dim=[2, 3])  # Global average pooling\n",
    "\n",
    "        # Reshape back to sequence\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        return features\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        # x shape: [batch_size, sequence_length, channels, height, width]\n",
    "\n",
    "        # Extract spatial features for each frame\n",
    "        spatial_features = self.extract_spatial_features(x)\n",
    "\n",
    "        if task == 'gaze':\n",
    "            # Temporal modeling with LSTM\n",
    "            lstm_out, _ = self.gaze_lstm(spatial_features)\n",
    "\n",
    "            # Apply temporal attention\n",
    "            lstm_out = lstm_out.transpose(0, 1)  # [seq_len, batch, features]\n",
    "            attn_out, _ = self.gaze_attention(lstm_out, lstm_out, lstm_out)\n",
    "            attn_out = attn_out.transpose(0, 1)  # [batch, seq_len, features]\n",
    "\n",
    "            # Aggregate temporal features (use last timestep or mean)\n",
    "            temporal_features = attn_out.mean(dim=1)  # Mean pooling over time\n",
    "\n",
    "            return self.gaze_head(temporal_features)\n",
    "\n",
    "        elif task == 'drowsy':\n",
    "            # Temporal modeling with LSTM\n",
    "            lstm_out, _ = self.drowsy_lstm(spatial_features)\n",
    "\n",
    "            # Apply temporal attention\n",
    "            lstm_out = lstm_out.transpose(0, 1)\n",
    "            attn_out, _ = self.drowsy_attention(lstm_out, lstm_out, lstm_out)\n",
    "            attn_out = attn_out.transpose(0, 1)\n",
    "\n",
    "            # For blinking detection, we might want to use the full sequence\n",
    "            temporal_features = attn_out.mean(dim=1)\n",
    "\n",
    "            return self.drowsy_head(temporal_features)\n",
    "\n",
    "        else:  # face\n",
    "            # Temporal modeling with LSTM\n",
    "            lstm_out, _ = self.face_lstm(spatial_features)\n",
    "\n",
    "            # Apply temporal attention\n",
    "            lstm_out = lstm_out.transpose(0, 1)\n",
    "            attn_out, _ = self.face_attention(lstm_out, lstm_out, lstm_out)\n",
    "            attn_out = attn_out.transpose(0, 1)\n",
    "\n",
    "            # Aggregate temporal features\n",
    "            temporal_features = attn_out.mean(dim=1)\n",
    "\n",
    "            return self.face_head(temporal_features)\n",
    "\n",
    "# ====== Temporal Dataset Wrapper for Validation/Test ======\n",
    "class TemporalMultiTaskDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.data = []\n",
    "        for task, ds in datasets.items():\n",
    "            self.data += [(task, i) for i in range(len(ds))]\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task, i = self.data[idx]\n",
    "        x, y = self.datasets[task][i]\n",
    "        return x, y, task\n",
    "\n",
    "# ====== Modified PCGrad for Temporal Model ======\n",
    "class TemporalPCGrad(torch.optim.Optimizer):\n",
    "    def __init__(self, optimizer):\n",
    "        self._optim = optimizer\n",
    "\n",
    "    def zero_grad(self):\n",
    "        return self._optim.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        return self._optim.step()\n",
    "\n",
    "    def pc_backward(self, objectives):\n",
    "        self._optim.zero_grad()\n",
    "\n",
    "        # Get parameter groups\n",
    "        backbone_params = self._optim.param_groups[0]['params']\n",
    "        lstm_params = self._optim.param_groups[1]['params']\n",
    "        head_params = self._optim.param_groups[2]['params']\n",
    "\n",
    "        grads = []\n",
    "        for i, loss in enumerate(objectives):\n",
    "            # Clear backbone and LSTM gradients\n",
    "            for p in backbone_params + lstm_params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad = None\n",
    "\n",
    "            loss.backward(retain_graph=(i < len(objectives) - 1))\n",
    "\n",
    "            # Collect gradients\n",
    "            single = []\n",
    "            for p in backbone_params + lstm_params:\n",
    "                if p.grad is None:\n",
    "                    single.append(torch.zeros_like(p))\n",
    "                else:\n",
    "                    single.append(p.grad.detach().clone())\n",
    "            grads.append(single)\n",
    "\n",
    "        # Apply PCGrad projection\n",
    "        proj = grads[0]\n",
    "        for other in grads[1:]:\n",
    "            for j, (g, g2) in enumerate(zip(proj, other)):\n",
    "                dot = (g * g2).sum()\n",
    "                if dot < 0:\n",
    "                    denom = g2.pow(2).sum() + 1e-12\n",
    "                    proj[j] = g - (dot / denom) * g2\n",
    "\n",
    "        # Write back projected gradients\n",
    "        for p, g in zip(backbone_params + lstm_params, proj):\n",
    "            p.grad = g\n",
    "\n",
    "# ====== Training Function for Temporal Model ======\n",
    "def train_temporal(model, optim, device, criterions, task_probs, loaders, steps_per_epoch, loss_scale=None):\n",
    "    if loss_scale is None:\n",
    "        loss_scale = {}\n",
    "\n",
    "    model.train()\n",
    "    iters = {t: iter(loaders[t]) for t in loaders}\n",
    "    total_loss = 0.0\n",
    "    task_losses = {t: 0.0 for t in loaders}\n",
    "    counts = {t: 0 for t in loaders}\n",
    "    corrects = {t: 0 for t in loaders}\n",
    "    totals = {t: 0 for t in loaders}\n",
    "\n",
    "    for _ in tqdm(range(steps_per_epoch), desc=\"Train\"):\n",
    "        objectives, batch_info = [], []\n",
    "        active_tasks = [t for t, p in task_probs.items() if p > 0]\n",
    "        random.shuffle(active_tasks)\n",
    "\n",
    "        for t in active_tasks:\n",
    "            try:\n",
    "                x, y = next(iters[t])\n",
    "            except StopIteration:\n",
    "                iters[t] = iter(loaders[t])\n",
    "                x, y = next(iters[t])\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, t)\n",
    "            loss = criterions[t](out, y)\n",
    "\n",
    "            if t in loss_scale:\n",
    "                loss = loss * float(loss_scale[t])\n",
    "\n",
    "            objectives.append(loss)\n",
    "            batch_info.append((t, out, y))\n",
    "\n",
    "        if not objectives:\n",
    "            continue\n",
    "\n",
    "        optim.zero_grad()\n",
    "        optim.pc_backward(objectives)\n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            step_loss = sum(l.item() for l in objectives) / len(objectives)\n",
    "            total_loss += step_loss\n",
    "\n",
    "            for (t, out, y), L in zip(batch_info, objectives):\n",
    "                preds = out.argmax(1)\n",
    "                corrects[t] += (preds == y).sum().item()\n",
    "                totals[t] += y.numel()\n",
    "                task_losses[t] += L.item() * y.size(0)\n",
    "                counts[t] += y.size(0)\n",
    "\n",
    "    avg_task_losses = {t: task_losses[t] / max(counts[t], 1) for t in task_losses}\n",
    "    accs = {t: 100 * corrects[t] / max(totals[t], 1) for t in totals}\n",
    "\n",
    "    return total_loss / steps_per_epoch, accs, avg_task_losses\n",
    "\n",
    "# ====== Create Optimizer for Temporal Model ======\n",
    "def create_temporal_optimizer(model, lr_backbone=1e-4, lr_lstm=1e-3, lr_heads=1e-3, weight_decay=1e-4):\n",
    "    # Group parameters\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "\n",
    "    lstm_params = []\n",
    "    for module in [model.gaze_lstm, model.drowsy_lstm, model.face_lstm,\n",
    "                   model.gaze_attention, model.drowsy_attention, model.face_attention]:\n",
    "        lstm_params.extend(list(module.parameters()))\n",
    "\n",
    "    head_params = []\n",
    "    for head in [model.gaze_head, model.drowsy_head, model.face_head]:\n",
    "        head_params.extend(list(head.parameters()))\n",
    "\n",
    "    base_optim = torch.optim.Adam([\n",
    "        {'params': backbone_params, 'lr': lr_backbone, 'weight_decay': weight_decay},\n",
    "        {'params': lstm_params, 'lr': lr_lstm, 'weight_decay': weight_decay},\n",
    "        {'params': head_params, 'lr': lr_heads, 'weight_decay': weight_decay},\n",
    "    ])\n",
    "\n",
    "    return TemporalPCGrad(base_optim)\n",
    "\n",
    "# ====== Main Training Script ======\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    # Configuration\n",
    "    SEQUENCE_LENGTH = 5  # Number of frames in temporal window\n",
    "    output_dir = \"temporal_training_results\"\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    # Data augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.15))\n",
    "    ])\n",
    "\n",
    "    # === Gaze Dataset (Temporal) ===\n",
    "    gaze_base = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Gaze/S6_face_RGB\"\n",
    "    subjects = list(range(1, 16))\n",
    "    random.seed(42)\n",
    "    random.shuffle(subjects)\n",
    "    train_subjects, val_subjects, test_subjects = subjects[:11], subjects[11:13], subjects[13:]\n",
    "\n",
    "    def build_temporal_gaze(subjects):\n",
    "        datasets = []\n",
    "        for s in subjects:\n",
    "            image_dir = os.path.join(gaze_base, str(s), \"Frames_RGB_Valid\")\n",
    "            csv_path = os.path.join(gaze_base, str(s), f\"Valid_gaze_label_{s}.csv\")\n",
    "            if os.path.exists(image_dir) and os.path.exists(csv_path):\n",
    "                datasets.append(TemporalGazeDataset(image_dir, csv_path, transform, SEQUENCE_LENGTH))\n",
    "        return ConcatDataset(datasets) if datasets else None\n",
    "\n",
    "    train_gaze = build_temporal_gaze(train_subjects)\n",
    "    val_gaze = build_temporal_gaze(val_subjects)\n",
    "    test_gaze = build_temporal_gaze(test_subjects)\n",
    "    print(\"Temporal Gaze data built\")\n",
    "\n",
    "    # === Drowsiness Dataset (Temporal - New Blinking Detection) ===\n",
    "    drowsy_base = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Drowsiness/S5\"\n",
    "\n",
    "    # Get all subject directories\n",
    "    drowsy_subjects = [os.path.join(drowsy_base, d) for d in os.listdir(drowsy_base)\n",
    "                      if os.path.isdir(os.path.join(drowsy_base, d)) and d.isdigit()]\n",
    "    drowsy_subjects.sort()\n",
    "\n",
    "    # Split subjects\n",
    "    random.seed(42)\n",
    "    random.shuffle(drowsy_subjects)\n",
    "    n_subjects = len(drowsy_subjects)\n",
    "    train_end = int(n_subjects * 0.7)\n",
    "    val_end = int(n_subjects * 0.85)\n",
    "\n",
    "    train_drowsy_subjects = drowsy_subjects[:train_end]\n",
    "    val_drowsy_subjects = drowsy_subjects[train_end:val_end]\n",
    "    test_drowsy_subjects = drowsy_subjects[val_end:]\n",
    "\n",
    "    train_drowsy = TemporalDrowsinessDataset(train_drowsy_subjects, transform, SEQUENCE_LENGTH)\n",
    "    val_drowsy = TemporalDrowsinessDataset(val_drowsy_subjects, transform, SEQUENCE_LENGTH)\n",
    "    test_drowsy = TemporalDrowsinessDataset(test_drowsy_subjects, transform, SEQUENCE_LENGTH)\n",
    "    print(f\"Temporal Drowsiness data built: Train={len(train_drowsy)}, Val={len(val_drowsy)}, Test={len(test_drowsy)}\")\n",
    "\n",
    "    # === Face Recognition Dataset (Temporal) ===\n",
    "    face_root = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Gaze/S6_face_RGB\"\n",
    "\n",
    "    def build_temporal_face_datasets(face_root, transform, sequence_length):\n",
    "        samples_train, samples_val, samples_test = [], [], []\n",
    "        pid_list = [f\"{i:02d}\" for i in range(1, 16)]\n",
    "        id_to_label = {pid: i for i, pid in enumerate(pid_list)}\n",
    "\n",
    "        for pid in pid_list:\n",
    "            label = id_to_label[pid]\n",
    "            img_dir = os.path.join(face_root, pid, \"Frames_RGB_Valid\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            img_paths = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "            img_paths.sort()\n",
    "\n",
    "            if len(img_paths) < sequence_length:\n",
    "                continue\n",
    "\n",
    "            train_imgs, temp_imgs = train_test_split(img_paths, test_size=0.3, random_state=42)\n",
    "            val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "            samples_train.extend([(p, label) for p in train_imgs])\n",
    "            samples_val.extend([(p, label) for p in val_imgs])\n",
    "            samples_test.extend([(p, label) for p in test_imgs])\n",
    "\n",
    "        return (\n",
    "            TemporalFaceDataset(samples_train, transform, sequence_length),\n",
    "            TemporalFaceDataset(samples_val, transform, sequence_length),\n",
    "            TemporalFaceDataset(samples_test, transform, sequence_length)\n",
    "        )\n",
    "\n",
    "    train_face, val_face, test_face = build_temporal_face_datasets(face_root, transform, SEQUENCE_LENGTH)\n",
    "    print(\"Temporal Face data built\")\n",
    "\n",
    "    # === Create DataLoaders ===\n",
    "    batch_size = 16  # Reduced due to temporal sequences\n",
    "\n",
    "    loaders = {\n",
    "        'gaze': DataLoader(train_gaze, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "        'drowsy': DataLoader(train_drowsy, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "        'face': DataLoader(train_face, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    }\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        TemporalMultiTaskDataset({'gaze': val_gaze, 'drowsy': val_drowsy, 'face': val_face}),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        TemporalMultiTaskDataset({'gaze': test_gaze, 'drowsy': test_drowsy, 'face': test_face}),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # === Model Setup ===\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = TemporalMultiTaskModel(\n",
    "        backbone_name='mobilevit_s',\n",
    "        pretrained=True,\n",
    "        gaze_classes=9,\n",
    "        drowsy_classes=2,  # Binary: blinking or not\n",
    "        face_classes=15,\n",
    "        sequence_length=SEQUENCE_LENGTH\n",
    "    ).to(device)\n",
    "\n",
    "    criterions = {\n",
    "        'gaze': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "        'drowsy': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "        'face': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    }\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = create_temporal_optimizer(\n",
    "        model,\n",
    "        lr_backbone=5e-5,\n",
    "        lr_lstm=1e-3,\n",
    "        lr_heads=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer._optim, step_size=10, gamma=0.5)\n",
    "\n",
    "    # === Training Loop ===\n",
    "    steps_per_epoch = max(len(loaders[t]) for t in loaders)\n",
    "    train_losses, val_losses = [], []\n",
    "    gaze_train_accs, gaze_val_accs = [], []\n",
    "    drowsy_train_accs, drowsy_val_accs = [], []\n",
    "    face_train_accs, face_val_accs = [], []\n",
    "    num_epochs = 30\n",
    "    gaze_task_losses_list, drowsy_task_losses_list, face_task_losses_list = [], [], []\n",
    "    best_val_loss = float('inf')\n",
    "    loss_scale = {'face': 0.25}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Staged training strategy\n",
    "        if epoch < 10:\n",
    "            # Stage 1: Focus on gaze with temporal features\n",
    "            task_probs = {'gaze': 1.0, 'drowsy': 0.0, 'face': 0.0}\n",
    "        elif epoch < 20:\n",
    "            # Stage 2: Add drowsiness (blinking detection)\n",
    "            task_probs = {'gaze': 0.5, 'drowsy': 0.5, 'face': 0.0}\n",
    "        else:\n",
    "            # Stage 3: All tasks\n",
    "            task_probs = {'gaze': 0.4, 'drowsy': 0.4, 'face': 0.2}\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} | task_probs={task_probs}\")\n",
    "\n",
    "        # Train\n",
    "        loss, accs, task_losses = train_temporal(\n",
    "            model, optimizer, device, criterions,\n",
    "            task_probs, loaders, steps_per_epoch, loss_scale\n",
    "        )\n",
    "\n",
    "        gaze_task_losses_list.append(task_losses['gaze'])\n",
    "        drowsy_task_losses_list.append(task_losses['drowsy'])\n",
    "        face_task_losses_list.append(task_losses['face'])\n",
    "\n",
    "        # Evaluate\n",
    "        def evaluate_temporal(model, loader, device, criterions):\n",
    "            model.eval()\n",
    "            preds = {t: [] for t in ['gaze', 'drowsy', 'face']}\n",
    "            targets = {t: [] for t in ['gaze', 'drowsy', 'face']}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, y, task in loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    for i, t in enumerate(task):\n",
    "                        out = model(x[i].unsqueeze(0), t)\n",
    "                        pred = out.argmax(dim=1).item()\n",
    "                        preds[t].append(pred)\n",
    "                        targets[t].append(y[i].item())\n",
    "\n",
    "            return {t: accuracy_score(targets[t], preds[t]) * 100 for t in preds}\n",
    "\n",
    "        def compute_val_loss_temporal(model, loader, device, criterions):\n",
    "            model.eval()\n",
    "            total_loss, total_samples = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x, y, task in loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    for i, t in enumerate(task):\n",
    "                        out = model(x[i].unsqueeze(0), t)\n",
    "                        loss = criterions[t](out, y[i].unsqueeze(0))\n",
    "                        total_loss += loss.item()\n",
    "                        total_samples += 1\n",
    "            return total_loss / max(total_samples, 1)\n",
    "\n",
    "        val_accs = evaluate_temporal(model, val_loader, device, criterions)\n",
    "        val_loss = compute_val_loss_temporal(model, val_loader, device, criterions)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} TrainLoss {loss:.4f} \"\n",
    "              f\"(Gaze {task_losses['gaze']:.4f} Drowsy {task_losses['drowsy']:.4f} Face {task_losses['face']:.4f}) \"\n",
    "              f\"ValLoss {val_loss:.4f} \"\n",
    "              f\"ValAcc Gaze {val_accs['gaze']:.2f}% Drowsy {val_accs['drowsy']:.2f}% Face {val_accs['face']:.2f}% \"\n",
    "              f\"LR {optimizer._optim.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_temporal_model.pth\")\n",
    "            print(f\"*** Best model updated at Epoch {epoch+1} with Val Loss {val_loss:.4f} ***\")\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "        gaze_train_accs.append(accs['gaze'])\n",
    "        gaze_val_accs.append(val_accs['gaze'])\n",
    "        drowsy_train_accs.append(accs['drowsy'])\n",
    "        drowsy_val_accs.append(val_accs['drowsy'])\n",
    "        face_train_accs.append(accs['face'])\n",
    "        face_val_accs.append(val_accs['face'])\n",
    "\n",
    "    # === Test with best model ===\n",
    "    print(\"\\nTesting with best temporal model...\")\n",
    "    model.load_state_dict(torch.load(\"best_temporal_model.pth\"))\n",
    "    test_accs = evaluate_temporal(model, test_loader, device, criterions)\n",
    "    print(\"Test Accuracies:\", test_accs)\n",
    "\n",
    "    # === Save Results ===\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ðŸŽ‰ TEMPORAL TRAINING COMPLETE! ðŸŽ‰\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Gaze Detection:      {test_accs['gaze']:.2f}%\")\n",
    "    print(f\"  Blinking Detection:  {test_accs['drowsy']:.2f}%\")\n",
    "    print(f\"  Face Recognition:    {test_accs['face']:.2f}%\")\n",
    "    print(f\"Best model saved: best_temporal_model.pth\")\n",
    "    print(f\"{'=' * 60}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikharsrivastava/Documents/envs/thesis/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Gaze data built\n",
      "Temporal Drowsiness data built: Train=54438, Val=10782, Test=16176\n",
      "Temporal Face data built\n",
      "Using device: mps\n",
      "\n",
      "Epoch 1/30 | task_probs={'gaze': 1.0, 'drowsy': 0.0, 'face': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   6%|â–Œ         | 194/3403 [03:39<1:00:46,  1.14s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb4a81fa47400456"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
