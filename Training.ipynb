{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:08:39.493687Z",
     "start_time": "2026-02-03T10:08:23.314943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_drowsiness_dataset(base_path):\n",
    "    \"\"\"Pre-scan the drowsiness dataset to identify problematic files\"\"\"\n",
    "    print(\"\\n=== Validating Drowsiness Dataset ===\")\n",
    "\n",
    "    subjects = [d for d in os.listdir(base_path)\n",
    "                if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()]\n",
    "\n",
    "    total_images = 0\n",
    "    missing_images = 0\n",
    "    corrupted_images = 0\n",
    "\n",
    "    for subject in subjects:\n",
    "        frames_dir = os.path.join(base_path, subject, \"frames\")\n",
    "        csv_path = os.path.join(frames_dir, \"labels.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"  No labels.csv in {subject}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            total_images += 1\n",
    "            img_path = os.path.join(frames_dir, row['filename'])\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                missing_images += 1\n",
    "                print(f\"  Missing: {img_path}\")\n",
    "            else:\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.verify()\n",
    "                except Exception as e:\n",
    "                    corrupted_images += 1\n",
    "                    print(f\"  Corrupted: {img_path}\")\n",
    "\n",
    "    print(f\"\\n  Dataset Statistics:\")\n",
    "    print(f\"    Total images referenced: {total_images}\")\n",
    "    print(f\"    Missing images: {missing_images}\")\n",
    "    print(f\"    Corrupted images: {corrupted_images}\")\n",
    "    print(f\"    Valid images: {total_images - missing_images - corrupted_images}\")\n",
    "    print(f\"    Data integrity: {(1 - (missing_images + corrupted_images) / total_images) * 100:.2f}%\")\n",
    "\n",
    "    return total_images, missing_images, corrupted_images\n",
    "\n",
    "# Call this before creating datasets (optional)\n",
    "validate_drowsiness_dataset(\"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Drowsiness/S5\")"
   ],
   "id": "1c13a84523fdd607",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating Drowsiness Dataset ===\n",
      "  Missing: /Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Drowsiness/S5/05/frames/01409.jpg\n",
      "\n",
      "  Dataset Statistics:\n",
      "    Total images referenced: 81456\n",
      "    Missing images: 1\n",
      "    Corrupted images: 0\n",
      "    Valid images: 81455\n",
      "    Data integrity: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81456, 1, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-02-03T10:08:46.222201Z"
    }
   },
   "source": [
    "import os, random, torch, glob\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ====== Original Single-Frame Datasets for Gaze and Face ======\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_index = self.labels_df.iloc[idx]['Frame Index']\n",
    "        gaze_label = int(self.labels_df.iloc[idx]['Gaze Zone'])\n",
    "        image_path = os.path.join(self.image_dir, f\"frame_{int(frame_index):04d}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, gaze_label\n",
    "\n",
    "class FaceRecognitionDataset(Dataset):\n",
    "    def __init__(self, samples, transform):\n",
    "        self.samples, self.transform = samples, transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ====== Temporal Dataset for Drowsiness (Blinking) ======\n",
    "# ====== Temporal Dataset for Drowsiness with Error Handling ======\n",
    "class TemporalDrowsinessDataset(Dataset):\n",
    "    \"\"\"Temporal dataset for blinking detection with robust error handling\"\"\"\n",
    "    def __init__(self, subject_dirs, transform=None, sequence_length=5, stride=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subject_dirs: List of subject directories\n",
    "            transform: Image transformations\n",
    "            sequence_length: Number of frames in temporal window\n",
    "            stride: Step size between sequences\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "\n",
    "        print(f\"  Building temporal dataset with window={sequence_length}, stride={stride}\")\n",
    "        total_sequences_attempted = 0\n",
    "        valid_sequences = 0\n",
    "        skipped_sequences = 0\n",
    "\n",
    "        for subject_dir in subject_dirs:\n",
    "            frames_dir = os.path.join(subject_dir, \"frames\")\n",
    "            csv_path = os.path.join(frames_dir, \"labels.csv\")\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"    Warning: No labels.csv found in {frames_dir}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df = df.sort_values('filename')\n",
    "\n",
    "            # Use stride for overlapping windows\n",
    "            for i in range(0, len(df) - sequence_length + 1, stride):\n",
    "                total_sequences_attempted += 1\n",
    "                seq_files = []\n",
    "                seq_labels = []\n",
    "                all_valid = True\n",
    "\n",
    "                # Check if all images in the sequence exist and are valid\n",
    "                for j in range(i, i + sequence_length):\n",
    "                    img_path = os.path.join(frames_dir, df.iloc[j]['filename'])\n",
    "\n",
    "                    # Check if file exists and can be opened\n",
    "                    if os.path.exists(img_path):\n",
    "                        try:\n",
    "                            # Try to open the image to verify it's valid\n",
    "                            with Image.open(img_path) as img:\n",
    "                                img.verify()  # Verify it's a valid image\n",
    "                            seq_files.append(img_path)\n",
    "                            seq_labels.append(df.iloc[j]['label'])\n",
    "                        except Exception as e:\n",
    "                            # Image exists but is corrupted\n",
    "                            all_valid = False\n",
    "                            break\n",
    "                    else:\n",
    "                        # Image doesn't exist\n",
    "                        all_valid = False\n",
    "                        break\n",
    "\n",
    "                if all_valid and len(seq_files) == sequence_length:\n",
    "                    # Use majority vote for label\n",
    "                    target_label = 1 if sum(seq_labels) > len(seq_labels) // 2 else 0\n",
    "                    self.samples.append((seq_files, target_label, seq_labels))\n",
    "                    valid_sequences += 1\n",
    "                else:\n",
    "                    skipped_sequences += 1\n",
    "\n",
    "        print(f\"  Dataset creation complete:\")\n",
    "        print(f\"    - Total sequences attempted: {total_sequences_attempted}\")\n",
    "        print(f\"    - Valid sequences created: {valid_sequences}\")\n",
    "        print(f\"    - Sequences skipped (missing/corrupted frames): {skipped_sequences}\")\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            print(f\"    WARNING: No valid sequences found! Check your data.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_files, label, seq_labels = self.samples[idx]\n",
    "        frames = []\n",
    "\n",
    "        # Load images with additional error handling\n",
    "        for img_path in seq_files:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                frames.append(image)\n",
    "            except Exception as e:\n",
    "                # If an image fails to load despite our checks,\n",
    "                # create a black frame as fallback\n",
    "                print(f\"    Warning: Failed to load {img_path} during training, using black frame\")\n",
    "                if self.transform:\n",
    "                    # Create a black image and transform it\n",
    "                    black_image = Image.new('RGB', (256, 256), color='black')\n",
    "                    image = self.transform(black_image)\n",
    "                else:\n",
    "                    # Create a tensor directly\n",
    "                    image = torch.zeros(3, 256, 256)\n",
    "                frames.append(image)\n",
    "\n",
    "        # Stack frames into temporal sequence\n",
    "        try:\n",
    "            frames = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        except Exception as e:\n",
    "            print(f\"    Error stacking frames: {e}\")\n",
    "            # Return a zero tensor as fallback\n",
    "            frames = torch.zeros(self.sequence_length, 3, 256, 256)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "# ====== Mixed Multi-Task Dataset ======\n",
    "class MixedMultiTaskDataset(Dataset):\n",
    "    \"\"\"Handles both single-frame and temporal data\"\"\"\n",
    "    def __init__(self, datasets):\n",
    "        self.data = []\n",
    "        for task, ds in datasets.items():\n",
    "            if ds is not None and len(ds) > 0:\n",
    "                self.data += [(task, i) for i in range(len(ds))]\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task, i = self.data[idx]\n",
    "        x, y = self.datasets[task][i]\n",
    "        return x, y, task\n",
    "\n",
    "# ====== Hybrid Multi-Task Model ======\n",
    "class HybridMultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone_name='mobilevit_s', pretrained=True,\n",
    "                 gaze_classes=9, drowsy_classes=2, face_classes=15,\n",
    "                 drowsy_sequence_length=5, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared spatial feature extractor\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.drowsy_sequence_length = drowsy_sequence_length\n",
    "\n",
    "        # Task-specific normalization\n",
    "        self.gaze_norm = nn.LayerNorm(feat_dim)\n",
    "        self.drowsy_norm = nn.LayerNorm(feat_dim)\n",
    "        self.face_norm = nn.LayerNorm(feat_dim)\n",
    "\n",
    "        # Temporal processing ONLY for drowsiness\n",
    "        # Lighter LSTM for faster training\n",
    "        self.drowsy_lstm = nn.LSTM(\n",
    "            feat_dim, hidden_dim // 2,  # Reduced hidden size\n",
    "            num_layers=1,  # Reduced layers for speed\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Simpler attention for speed\n",
    "        self.drowsy_attention = nn.MultiheadAttention(\n",
    "            hidden_dim,  # Matches LSTM output\n",
    "            num_heads=2,  # Reduced heads\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.gaze_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, gaze_classes)\n",
    "        )\n",
    "\n",
    "        self.drowsy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),  # Smaller head\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, drowsy_classes)\n",
    "        )\n",
    "\n",
    "        self.face_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, face_classes)\n",
    "        )\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extract features handling both single images and sequences\"\"\"\n",
    "        if len(x.shape) == 4:  # Single image [B, C, H, W]\n",
    "            features = self.backbone.forward_features(x)\n",
    "            features = features.mean(dim=[2, 3])  # GAP\n",
    "            return features\n",
    "        else:  # Sequence [B, T, C, H, W]\n",
    "            batch_size, seq_len, c, h, w = x.size()\n",
    "            x_flat = x.view(batch_size * seq_len, c, h, w)\n",
    "            features = self.backbone.forward_features(x_flat)\n",
    "            features = features.mean(dim=[2, 3])  # GAP\n",
    "            features = features.view(batch_size, seq_len, -1)\n",
    "            return features\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        if task == 'gaze':\n",
    "            # Single-frame processing\n",
    "            features = self.extract_features(x)\n",
    "            features = self.gaze_norm(features)\n",
    "            return self.gaze_head(features)\n",
    "\n",
    "        elif task == 'drowsy':\n",
    "            # Temporal processing for blinking detection\n",
    "            features = self.extract_features(x)  # [B, T, feat_dim]\n",
    "            features = self.drowsy_norm(features)\n",
    "\n",
    "            # LSTM for temporal modeling\n",
    "            lstm_out, _ = self.drowsy_lstm(features)\n",
    "\n",
    "            # Simple temporal pooling (faster than attention)\n",
    "            # You can switch between mean pooling and attention\n",
    "            USE_ATTENTION = False  # Toggle for speed vs accuracy\n",
    "\n",
    "            if USE_ATTENTION:\n",
    "                lstm_out = lstm_out.transpose(0, 1)\n",
    "                attn_out, _ = self.drowsy_attention(lstm_out, lstm_out, lstm_out)\n",
    "                temporal_features = attn_out.mean(dim=0)  # [B, hidden_dim]\n",
    "            else:\n",
    "                # Simple mean pooling (faster)\n",
    "                temporal_features = lstm_out.mean(dim=1)  # [B, hidden_dim]\n",
    "\n",
    "            return self.drowsy_head(temporal_features)\n",
    "\n",
    "        else:  # face\n",
    "            # Single-frame processing\n",
    "            features = self.extract_features(x)\n",
    "            features = self.face_norm(features)\n",
    "            return self.face_head(features)\n",
    "\n",
    "# ====== PCGrad with Dynamic Weight Decay ======\n",
    "class PCGradWithDynamicWD(torch.optim.Optimizer):\n",
    "    def __init__(self, optimizer):\n",
    "        self._optim = optimizer\n",
    "        self.current_stage = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        return self._optim.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        return self._optim.step()\n",
    "\n",
    "    def update_weight_decay(self, stage, task_probs):\n",
    "        \"\"\"Dynamically adjust weight decay based on training stage\"\"\"\n",
    "        self.current_stage = stage\n",
    "\n",
    "        # Define weight decay strategies for different stages\n",
    "        wd_strategies = {\n",
    "            0: {  # Stage 1: Single task (Gaze only)\n",
    "                'backbone': 1e-4,\n",
    "                'temporal': 1e-4,\n",
    "                'heads': 1e-4\n",
    "            },\n",
    "            1: {  # Stage 2: Gaze + Drowsy\n",
    "                'backbone': 5e-4,  # Increase to prevent overfitting\n",
    "                'temporal': 1e-3,  # Higher for LSTM\n",
    "                'heads': 5e-4\n",
    "            },\n",
    "            2: {  # Stage 3: All tasks\n",
    "                'backbone': 1e-3,  # Further increase\n",
    "                'temporal': 2e-3,  # Highest for temporal\n",
    "                'heads': 1e-3\n",
    "            }\n",
    "        }\n",
    "\n",
    "        wd_config = wd_strategies.get(stage, wd_strategies[2])\n",
    "\n",
    "        # Apply to parameter groups\n",
    "        if len(self._optim.param_groups) >= 3:\n",
    "            self._optim.param_groups[0]['weight_decay'] = wd_config['backbone']\n",
    "            self._optim.param_groups[1]['weight_decay'] = wd_config['temporal']\n",
    "            self._optim.param_groups[2]['weight_decay'] = wd_config['heads']\n",
    "\n",
    "        print(f\"  Weight Decay Updated - Backbone: {wd_config['backbone']}, \"\n",
    "              f\"Temporal: {wd_config['temporal']}, Heads: {wd_config['heads']}\")\n",
    "\n",
    "    def update_learning_rates(self, stage):\n",
    "        \"\"\"Adjust learning rates based on stage\"\"\"\n",
    "        lr_strategies = {\n",
    "            0: {'backbone': 1e-4, 'temporal': 1e-3, 'heads': 1e-3},\n",
    "            1: {'backbone': 5e-5, 'temporal': 5e-4, 'heads': 5e-4},\n",
    "            2: {'backbone': 1e-5, 'temporal': 1e-4, 'heads': 1e-4}\n",
    "        }\n",
    "\n",
    "        lr_config = lr_strategies.get(stage, lr_strategies[2])\n",
    "\n",
    "        if len(self._optim.param_groups) >= 3:\n",
    "            self._optim.param_groups[0]['lr'] = lr_config['backbone']\n",
    "            self._optim.param_groups[1]['lr'] = lr_config['temporal']\n",
    "            self._optim.param_groups[2]['lr'] = lr_config['heads']\n",
    "\n",
    "        print(f\"  Learning Rates Updated - Backbone: {lr_config['backbone']}, \"\n",
    "              f\"Temporal: {lr_config['temporal']}, Heads: {lr_config['heads']}\")\n",
    "\n",
    "    def pc_backward(self, objectives):\n",
    "        self._optim.zero_grad()\n",
    "\n",
    "        backbone_params = self._optim.param_groups[0]['params']\n",
    "        temporal_params = self._optim.param_groups[1]['params'] if len(self._optim.param_groups) > 1 else []\n",
    "\n",
    "        grads = []\n",
    "        for i, loss in enumerate(objectives):\n",
    "            for p in backbone_params + temporal_params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad = None\n",
    "\n",
    "            loss.backward(retain_graph=(i < len(objectives) - 1))\n",
    "\n",
    "            single = []\n",
    "            for p in backbone_params + temporal_params:\n",
    "                if p.grad is None:\n",
    "                    single.append(torch.zeros_like(p))\n",
    "                else:\n",
    "                    single.append(p.grad.detach().clone())\n",
    "            grads.append(single)\n",
    "\n",
    "        proj = grads[0]\n",
    "        for other in grads[1:]:\n",
    "            for j, (g, g2) in enumerate(zip(proj, other)):\n",
    "                dot = (g * g2).sum()\n",
    "                if dot < 0:\n",
    "                    denom = g2.pow(2).sum() + 1e-12\n",
    "                    proj[j] = g - (dot / denom) * g2\n",
    "\n",
    "        for p, g in zip(backbone_params + temporal_params, proj):\n",
    "            p.grad = g\n",
    "\n",
    "# ====== Create Optimizer ======\n",
    "def create_hybrid_optimizer(model, lr_backbone=1e-4, lr_temporal=1e-3, lr_heads=1e-3):\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "\n",
    "    temporal_params = []\n",
    "    temporal_modules = [model.drowsy_lstm, model.drowsy_attention]\n",
    "    for module in temporal_modules:\n",
    "        temporal_params.extend(list(module.parameters()))\n",
    "\n",
    "    head_params = []\n",
    "    heads = [model.gaze_head, model.drowsy_head, model.face_head]\n",
    "    norms = [model.gaze_norm, model.drowsy_norm, model.face_norm]\n",
    "    for module in heads + norms:\n",
    "        head_params.extend(list(module.parameters()))\n",
    "\n",
    "    base_optim = torch.optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': lr_backbone, 'weight_decay': 1e-4},\n",
    "        {'params': temporal_params, 'lr': lr_temporal, 'weight_decay': 1e-4},\n",
    "        {'params': head_params, 'lr': lr_heads, 'weight_decay':1e-4}\n",
    "    ])\n",
    "\n",
    "    return PCGradWithDynamicWD(base_optim)  # Fixed typo here\n",
    "\n",
    "# ====== Training Function ======\n",
    "def train_hybrid(model, optim, device, criterions, task_probs, loaders, steps_per_epoch, loss_scale=None):\n",
    "    if loss_scale is None:\n",
    "        loss_scale = {}\n",
    "\n",
    "    model.train()\n",
    "    iters = {t: iter(loaders[t]) for t in loaders if loaders[t] is not None}\n",
    "    total_loss = 0.0\n",
    "    task_losses = {t: 0.0 for t in loaders}\n",
    "    counts = {t: 0 for t in loaders}\n",
    "    corrects = {t: 0 for t in loaders}\n",
    "    totals = {t: 0 for t in loaders}\n",
    "\n",
    "    for _ in tqdm(range(steps_per_epoch), desc=\"Train\"):\n",
    "        objectives, batch_info = [], []\n",
    "        active_tasks = [t for t, p in task_probs.items() if p > 0 and t in iters]\n",
    "        random.shuffle(active_tasks)\n",
    "\n",
    "        for t in active_tasks:\n",
    "            try:\n",
    "                x, y = next(iters[t])\n",
    "            except StopIteration:\n",
    "                iters[t] = iter(loaders[t])\n",
    "                x, y = next(iters[t])\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, t)\n",
    "            loss = criterions[t](out, y)\n",
    "\n",
    "            if t in loss_scale:\n",
    "                loss = loss * float(loss_scale[t])\n",
    "\n",
    "            objectives.append(loss)\n",
    "            batch_info.append((t, out, y))\n",
    "\n",
    "        if not objectives:\n",
    "            continue\n",
    "\n",
    "        optim.zero_grad()\n",
    "        optim.pc_backward(objectives)\n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            step_loss = sum(l.item() for l in objectives) / len(objectives)\n",
    "            total_loss += step_loss\n",
    "\n",
    "            for (t, out, y), L in zip(batch_info, objectives):\n",
    "                preds = out.argmax(1)\n",
    "                corrects[t] += (preds == y).sum().item()\n",
    "                totals[t] += y.numel()\n",
    "                task_losses[t] += L.item() * y.size(0)\n",
    "                counts[t] += y.size(0)\n",
    "\n",
    "    avg_task_losses = {t: task_losses[t] / max(counts[t], 1) for t in task_losses}\n",
    "    accs = {t: 100 * corrects[t] / max(totals[t], 1) for t in totals}\n",
    "\n",
    "    return total_loss / steps_per_epoch, accs, avg_task_losses\n",
    "\n",
    "# ====== Evaluation Function ======\n",
    "def evaluate_hybrid(model, loader, device, criterions):\n",
    "    model.eval()\n",
    "    preds = {t: [] for t in ['gaze', 'drowsy', 'face']}\n",
    "    targets = {t: [] for t in ['gaze', 'drowsy', 'face']}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, task in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            for i, t in enumerate(task):\n",
    "                out = model(x[i].unsqueeze(0), t)\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                preds[t].append(pred)\n",
    "                targets[t].append(y[i].item())\n",
    "\n",
    "    return {t: accuracy_score(targets[t], preds[t]) * 100 if len(preds[t]) > 0 else 0 for t in preds}\n",
    "\n",
    "# ====== Main Training Script ======\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    # ====== WINDOW SIZE ANALYSIS ======\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WINDOW SIZE EFFECTS ON TRAINING SPEED:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    Window Size | Memory Usage | Training Speed | Dataset Size | Accuracy\n",
    "    ------------|--------------|----------------|--------------|----------\n",
    "    3 frames    | Low (3x)     | Fast           | Large        | Lower\n",
    "    5 frames    | Medium (5x)  | Moderate       | Medium       | Good\n",
    "    7 frames    | High (7x)    | Slower         | Smaller      | Better\n",
    "    9 frames    | V.High (9x)  | Slow           | Small        | Best*\n",
    "\n",
    "    * Diminishing returns after 7-9 frames for blinking detection\n",
    "\n",
    "    Trade-offs:\n",
    "    - Larger window = More temporal context = Better accuracy\n",
    "    - Larger window = More memory = Smaller batch size = Slower training\n",
    "    - Larger window = Fewer sequences (with same stride) = Less data variety\n",
    "\n",
    "    Recommended: 5-7 frames for blinking (captures full blink cycle ~200-300ms)\n",
    "    \"\"\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Configuration\n",
    "    DROWSY_WINDOW_SIZE = 5  # Optimal balance (you can experiment: 3, 5, 7, 9)\n",
    "    DROWSY_STRIDE = 2  # Overlap between windows (1=max overlap, window_size=no overlap)\n",
    "\n",
    "    output_dir = \"hybrid_training_results\"\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    # Data augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.15))\n",
    "    ])\n",
    "\n",
    "    # === SUBJECT-WISE SPLITS FOR ALL DATASETS ===\n",
    "\n",
    "    # === Gaze Dataset (Subject-wise split) ===\n",
    "    print(\"\\n=== Setting up Gaze Dataset ===\")\n",
    "    gaze_base = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Gaze/S6_face_RGB\"\n",
    "    gaze_subjects = list(range(1, 16))\n",
    "    random.seed(42)\n",
    "    random.shuffle(gaze_subjects)\n",
    "\n",
    "    # 70-15-15 split\n",
    "    n_train_gaze = int(len(gaze_subjects) * 0.7)\n",
    "    n_val_gaze = int(len(gaze_subjects) * 0.15)\n",
    "\n",
    "    train_gaze_subjects = gaze_subjects[:n_train_gaze]\n",
    "    val_gaze_subjects = gaze_subjects[n_train_gaze:n_train_gaze + n_val_gaze]\n",
    "    test_gaze_subjects = gaze_subjects[n_train_gaze + n_val_gaze:]\n",
    "\n",
    "    print(f\"Gaze subjects - Train: {train_gaze_subjects}, Val: {val_gaze_subjects}, Test: {test_gaze_subjects}\")\n",
    "\n",
    "    def build_gaze(subjects):\n",
    "        datasets = []\n",
    "        for s in subjects:\n",
    "            img_dir = os.path.join(gaze_base, str(s), \"Frames_RGB_Valid\")\n",
    "            csv_path = os.path.join(gaze_base, str(s), f\"Valid_gaze_label_{s}.csv\")\n",
    "            if os.path.exists(img_dir) and os.path.exists(csv_path):\n",
    "                datasets.append(GazeDataset(img_dir, csv_path, transform))\n",
    "        return ConcatDataset(datasets) if datasets else None\n",
    "\n",
    "    train_gaze = build_gaze(train_gaze_subjects)\n",
    "    val_gaze = build_gaze(val_gaze_subjects)  # Fixed typo here\n",
    "    test_gaze = build_gaze(test_gaze_subjects)\n",
    "    print(f\"Gaze data built - Train: {len(train_gaze) if train_gaze else 0}, \"\n",
    "          f\"Val: {len(val_gaze) if val_gaze else 0}, \"\n",
    "          f\"Test: {len(test_gaze) if test_gaze else 0}\")\n",
    "\n",
    "    # === Drowsiness Dataset (Subject-wise split) ===\n",
    "    print(\"\\n=== Setting up Drowsiness Dataset ===\")\n",
    "    drowsy_base = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Drowsiness/S5\"\n",
    "\n",
    "    # Get all subject directories (numbered folders)\n",
    "    drowsy_subjects = [\n",
    "        d for d in os.listdir(drowsy_base)\n",
    "        if os.path.isdir(os.path.join(drowsy_base, d)) and d.isdigit()\n",
    "    ]\n",
    "    drowsy_subjects = sorted(drowsy_subjects, key=int)  # Sort numerically\n",
    "\n",
    "    print(f\"Found {len(drowsy_subjects)} drowsiness subjects: {drowsy_subjects}\")\n",
    "\n",
    "    # Subject-wise split (70-15-15)\n",
    "    random.seed(42)\n",
    "    random.shuffle(drowsy_subjects)\n",
    "\n",
    "    n_train_drowsy = int(len(drowsy_subjects) * 0.7)\n",
    "    n_val_drowsy = int(len(drowsy_subjects) * 0.15)\n",
    "\n",
    "    train_drowsy_subjects = drowsy_subjects[:n_train_drowsy]\n",
    "    val_drowsy_subjects = drowsy_subjects[n_train_drowsy:n_train_drowsy + n_val_drowsy]\n",
    "    test_drowsy_subjects = drowsy_subjects[n_train_drowsy + n_val_drowsy:]\n",
    "\n",
    "    print(f\"Drowsy subjects - Train: {train_drowsy_subjects}, Val: {val_drowsy_subjects}, Test: {test_drowsy_subjects}\")\n",
    "\n",
    "    # Convert subject IDs to full paths\n",
    "    train_drowsy_dirs = [os.path.join(drowsy_base, s) for s in train_drowsy_subjects]\n",
    "    val_drowsy_dirs = [os.path.join(drowsy_base, s) for s in val_drowsy_subjects]\n",
    "    test_drowsy_dirs = [os.path.join(drowsy_base, s) for s in test_drowsy_subjects]\n",
    "\n",
    "    # Create temporal datasets\n",
    "    print(f\"\\nCreating temporal drowsiness datasets (window={DROWSY_WINDOW_SIZE}, stride={DROWSY_STRIDE})...\")\n",
    "\n",
    "    train_drowsy = TemporalDrowsinessDataset(\n",
    "        train_drowsy_dirs, transform, DROWSY_WINDOW_SIZE, DROWSY_STRIDE\n",
    "    )\n",
    "    val_drowsy = TemporalDrowsinessDataset(\n",
    "        val_drowsy_dirs, transform, DROWSY_WINDOW_SIZE, DROWSY_STRIDE\n",
    "    )\n",
    "    test_drowsy = TemporalDrowsinessDataset(\n",
    "        test_drowsy_dirs, transform, DROWSY_WINDOW_SIZE, DROWSY_STRIDE\n",
    "    )\n",
    "\n",
    "    print(f\"Drowsiness data built - Train: {len(train_drowsy)}, Val: {len(val_drowsy)}, Test: {len(test_drowsy)}\")\n",
    "\n",
    "    # === Face Recognition Dataset (Subject-wise split) ===\n",
    "    print(\"\\n=== Setting up Face Recognition Dataset ===\")\n",
    "    face_root = \"/Users/shikharsrivastava/Desktop/Thesis/Thesis/MultiTask/Gaze/S6_face_RGB\"\n",
    "\n",
    "    # Same subjects as gaze (they're the same dataset)\n",
    "    def build_face_datasets(subjects, transform):\n",
    "        samples = []\n",
    "        pid_list = [f\"{i:02d}\" for i in subjects]\n",
    "        id_to_label = {pid: i for i, pid in enumerate([f\"{j:02d}\" for j in range(1, 16)])}\n",
    "\n",
    "        for pid in pid_list:\n",
    "            label = id_to_label[pid]\n",
    "            img_dir = os.path.join(face_root, pid, \"Frames_RGB_Valid\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            img_paths = [\n",
    "                os.path.join(img_dir, f) for f in os.listdir(img_dir)\n",
    "                if f.endswith(\".jpg\")\n",
    "            ]\n",
    "            img_paths.sort()\n",
    "            samples.extend([(p, label) for p in img_paths])\n",
    "\n",
    "        return FaceRecognitionDataset(samples, transform) if samples else None\n",
    "\n",
    "    train_face = build_face_datasets(train_gaze_subjects, transform)\n",
    "    val_face = build_face_datasets(val_gaze_subjects, transform)\n",
    "    test_face = build_face_datasets(test_gaze_subjects, transform)\n",
    "\n",
    "    print(f\"Face data built - Train: {len(train_face) if train_face else 0}, \"\n",
    "          f\"Val: {len(val_face) if val_face else 0}, \"\n",
    "          f\"Test: {len(test_face) if test_face else 0}\")\n",
    "\n",
    "    # === Create DataLoaders ===\n",
    "    # Adjust batch size based on window size\n",
    "    base_batch_size = 32\n",
    "    temporal_batch_size = max(4, base_batch_size // DROWSY_WINDOW_SIZE)  # Scale down for memory\n",
    "\n",
    "    print(f\"\\nBatch sizes - Regular: {base_batch_size}, Temporal: {temporal_batch_size}\")\n",
    "\n",
    "    loaders = {}\n",
    "    if train_gaze and len(train_gaze) > 0:\n",
    "        loaders['gaze'] = DataLoader(train_gaze, batch_size=base_batch_size, shuffle=True, num_workers=0)\n",
    "    if train_drowsy and len(train_drowsy) > 0:\n",
    "        loaders['drowsy'] = DataLoader(train_drowsy, batch_size=temporal_batch_size, shuffle=True, num_workers=0)\n",
    "    if train_face and len(train_face) > 0:\n",
    "        loaders['face'] = DataLoader(train_face, batch_size=base_batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Validation and test loaders\n",
    "    val_datasets = {}\n",
    "    test_datasets = {}\n",
    "\n",
    "    if val_gaze and len(val_gaze) > 0:\n",
    "        val_datasets['gaze'] = val_gaze\n",
    "    if val_drowsy and len(val_drowsy) > 0:\n",
    "        val_datasets['drowsy'] = val_drowsy\n",
    "    if val_face and len(val_face) > 0:\n",
    "        val_datasets['face'] = val_face\n",
    "\n",
    "    if test_gaze and len(test_gaze) > 0:\n",
    "        test_datasets['gaze'] = test_gaze\n",
    "    if test_drowsy and len(test_drowsy) > 0:\n",
    "        test_datasets['drowsy'] = test_drowsy\n",
    "    if test_face and len(test_face) > 0:\n",
    "        test_datasets['face'] = test_face\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        MixedMultiTaskDataset(val_datasets),\n",
    "        batch_size=1  # Process one at a time due to mixed dimensions\n",
    "    ) if val_datasets else None\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MixedMultiTaskDataset(test_datasets),\n",
    "        batch_size=1\n",
    "    ) if test_datasets else None\n",
    "\n",
    "    # === Model Setup ===\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    model = HybridMultiTaskModel(\n",
    "        backbone_name='mobilevit_s',\n",
    "        pretrained=True,\n",
    "        gaze_classes=9,\n",
    "        drowsy_classes=2,  # Binary: blinking or not\n",
    "        face_classes=15,\n",
    "        drowsy_sequence_length=DROWSY_WINDOW_SIZE\n",
    "    ).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    criterions = {\n",
    "        'gaze': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "        'drowsy': nn.CrossEntropyLoss(label_smoothing=0.05, weight=torch.tensor([1.0, 2.0]).to(device)),  # Weight blinking class higher\n",
    "        'face': nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    }\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = create_hybrid_optimizer(model)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer._optim, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # === Training Loop ===\n",
    "    steps_per_epoch = max(len(loaders[t]) for t in loaders) if loaders else 100\n",
    "    train_losses, val_losses = [], []\n",
    "    gaze_train_accs, gaze_val_accs = [], []\n",
    "    drowsy_train_accs, drowsy_val_accs = [], []\n",
    "    face_train_accs, face_val_accs = [], []\n",
    "    num_epochs = 20\n",
    "    gaze_task_losses_list, drowsy_task_losses_list, face_task_losses_list = [], [], []\n",
    "    best_val_loss = float('inf')\n",
    "    loss_scale = {'face': 0.3, 'drowsy': 1.5}  # Boost drowsy loss\n",
    "\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # === Staged Training with Dynamic Weight Decay ===\n",
    "        if epoch < 8:\n",
    "            # Stage 1: Focus on gaze (single-frame)\n",
    "            stage = 0\n",
    "            task_probs = {'gaze': 1, 'drowsy': 0.0, 'face': 0.0}\n",
    "            if epoch == 0:\n",
    "                optimizer.update_weight_decay(stage, task_probs)\n",
    "                optimizer.update_learning_rates(stage)\n",
    "\n",
    "        elif epoch < 18:\n",
    "            # Stage 2: Add temporal drowsiness\n",
    "            stage = 1\n",
    "            task_probs = {'gaze': 0.7, 'drowsy': 0.3, 'face': 0.0}\n",
    "            if epoch == 8:\n",
    "                optimizer.update_weight_decay(stage, task_probs)\n",
    "                optimizer.update_learning_rates(stage)\n",
    "\n",
    "        else:\n",
    "            # Stage 3: All tasks\n",
    "            stage = 2\n",
    "            task_probs = {'gaze': 0.65, 'drowsy': 0.25, 'face': 0.1}\n",
    "            if epoch == 18:\n",
    "                optimizer.update_weight_decay(stage, task_probs)\n",
    "                optimizer.update_learning_rates(stage)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} | Stage {stage+1} | task_probs={task_probs}\")\n",
    "\n",
    "        # Train\n",
    "        loss, accs, task_losses = train_hybrid(\n",
    "            model, optimizer, device, criterions,\n",
    "            task_probs, loaders, steps_per_epoch, loss_scale\n",
    "        )\n",
    "\n",
    "        gaze_task_losses_list.append(task_losses.get('gaze', 0))\n",
    "        drowsy_task_losses_list.append(task_losses.get('drowsy', 0))\n",
    "        face_task_losses_list.append(task_losses.get('face', 0))\n",
    "\n",
    "        # Evaluate\n",
    "        if val_loader:\n",
    "            val_accs = evaluate_hybrid(model, val_loader, device, criterions)\n",
    "\n",
    "            # Compute validation loss\n",
    "            def compute_val_loss_hybrid(model, loader, device, criterions):\n",
    "                model.eval()\n",
    "                total_loss, total_samples = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for x, y, task in loader:\n",
    "                        x, y = x.to(device), y.to(device)\n",
    "                        for i, t in enumerate(task):\n",
    "                            out = model(x[i].unsqueeze(0), t)\n",
    "                            loss = criterions[t](out, y[i].unsqueeze(0))\n",
    "                            total_loss += loss.item()\n",
    "                            total_samples += 1\n",
    "                return total_loss / max(total_samples, 1)\n",
    "\n",
    "            val_loss = compute_val_loss_hybrid(model, val_loader, device, criterions)\n",
    "        else:\n",
    "            val_accs = {'gaze': 0, 'drowsy': 0, 'face': 0}\n",
    "            val_loss = float('inf')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} TrainLoss {loss:.4f} \"\n",
    "              f\"(Gaze {task_losses.get('gaze', 0):.4f} Drowsy {task_losses.get('drowsy', 0):.4f} Face {task_losses.get('face', 0):.4f}) \"\n",
    "              f\"ValLoss {val_loss:.4f} \"\n",
    "              f\"ValAcc Gaze {val_accs['gaze']:.2f}% Drowsy {val_accs['drowsy']:.2f}% Face {val_accs['face']:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_hybrid_model.pth\")\n",
    "            print(f\"*** Best model updated at Epoch {epoch+1} with Val Loss {val_loss:.4f} ***\")\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "        gaze_train_accs.append(accs.get('gaze', 0))\n",
    "        gaze_val_accs.append(val_accs['gaze'])\n",
    "        drowsy_train_accs.append(accs.get('drowsy', 0))\n",
    "        drowsy_val_accs.append(val_accs['drowsy'])\n",
    "        face_train_accs.append(accs.get('face', 0))\n",
    "        face_val_accs.append(val_accs['face'])\n",
    "\n",
    "    # === Test with best model ===\n",
    "    print(\"\\nTesting with best hybrid model...\")\n",
    "    if os.path.exists(\"best_hybrid_model.pth\"):\n",
    "        model.load_state_dict(torch.load(\"best_hybrid_model.pth\"))\n",
    "\n",
    "    if test_loader:\n",
    "        test_accs = evaluate_hybrid(model, test_loader, device, criterions)\n",
    "    else:\n",
    "        test_accs = {'gaze': 0, 'drowsy': 0, 'face': 0}\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ðŸŽ‰ HYBRID TRAINING COMPLETE! ðŸŽ‰\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Temporal Window Size: {DROWSY_WINDOW_SIZE} frames\")\n",
    "    print(f\"  Temporal Stride: {DROWSY_STRIDE}\")\n",
    "    print(f\"  Subject-wise splits for all tasks\")\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Gaze Detection (Single-frame):     {test_accs['gaze']:.2f}%\")\n",
    "    print(f\"  Blinking Detection (Temporal):     {test_accs['drowsy']:.2f}%\")\n",
    "    print(f\"  Face Recognition (Single-frame):   {test_accs['face']:.2f}%\")\n",
    "    print(f\"\\nBest model saved: best_hybrid_model.pth\")\n",
    "    print(f\"{'=' * 60}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WINDOW SIZE EFFECTS ON TRAINING SPEED:\n",
      "============================================================\n",
      "\n",
      "    Window Size | Memory Usage | Training Speed | Dataset Size | Accuracy\n",
      "    ------------|--------------|----------------|--------------|----------\n",
      "    3 frames    | Low (3x)     | Fast           | Large        | Lower\n",
      "    5 frames    | Medium (5x)  | Moderate       | Medium       | Good\n",
      "    7 frames    | High (7x)    | Slower         | Smaller      | Better\n",
      "    9 frames    | V.High (9x)  | Slow           | Small        | Best*\n",
      "\n",
      "    * Diminishing returns after 7-9 frames for blinking detection\n",
      "\n",
      "    Trade-offs:\n",
      "    - Larger window = More temporal context = Better accuracy\n",
      "    - Larger window = More memory = Smaller batch size = Slower training\n",
      "    - Larger window = Fewer sequences (with same stride) = Less data variety\n",
      "\n",
      "    Recommended: 5-7 frames for blinking (captures full blink cycle ~200-300ms)\n",
      "    \n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Setting up Gaze Dataset ===\n",
      "Gaze subjects - Train: [9, 14, 8, 7, 15, 13, 6, 3, 10, 4], Val: [5, 12], Test: [1, 2, 11]\n",
      "Gaze data built - Train: 23704, Val: 4956, Test: 7039\n",
      "\n",
      "=== Setting up Drowsiness Dataset ===\n",
      "Found 15 drowsiness subjects: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
      "Drowsy subjects - Train: ['09', '14', '08', '07', '15', '13', '06', '03', '10', '04'], Val: ['05', '12'], Test: ['01', '02', '11']\n",
      "\n",
      "Creating temporal drowsiness datasets (window=5, stride=2)...\n",
      "  Building temporal dataset with window=5, stride=2\n",
      "  Dataset creation complete:\n",
      "    - Total sequences attempted: 27221\n",
      "    - Valid sequences created: 27221\n",
      "    - Sequences skipped (missing/corrupted frames): 0\n",
      "  Building temporal dataset with window=5, stride=2\n",
      "  Dataset creation complete:\n",
      "    - Total sequences attempted: 5392\n",
      "    - Valid sequences created: 5390\n",
      "    - Sequences skipped (missing/corrupted frames): 2\n",
      "  Building temporal dataset with window=5, stride=2\n",
      "  Dataset creation complete:\n",
      "    - Total sequences attempted: 8088\n",
      "    - Valid sequences created: 8088\n",
      "    - Sequences skipped (missing/corrupted frames): 0\n",
      "Drowsiness data built - Train: 27221, Val: 5390, Test: 8088\n",
      "\n",
      "=== Setting up Face Recognition Dataset ===\n",
      "Face data built - Train: 10114, Val: 2522, Test: 2317\n",
      "\n",
      "Batch sizes - Regular: 32, Temporal: 6\n",
      "\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikharsrivastava/Documents/envs/thesis/lib/python3.13/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6,344,058\n",
      "Trainable parameters: 6,344,058\n",
      "\n",
      "Starting training for 20 epochs...\n",
      "Steps per epoch: 4537\n",
      "  Weight Decay Updated - Backbone: 0.0001, Temporal: 0.0001, Heads: 0.0001\n",
      "  Learning Rates Updated - Backbone: 0.0001, Temporal: 0.001, Heads: 0.001\n",
      "\n",
      "Epoch 1/20 | Stage 1 | task_probs={'gaze': 1, 'drowsy': 0.0, 'face': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  22%|â–ˆâ–ˆâ–       | 1009/4537 [07:37<26:55,  2.18it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb4a81fa47400456"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
